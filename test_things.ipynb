{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oid</th>\n",
       "      <th>mjd</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>fid</th>\n",
       "      <th>flux_tot_ujy</th>\n",
       "      <th>fluxunc_tot_ujy_resc</th>\n",
       "      <th>alerceclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>ZTF18aaaqexr</td>\n",
       "      <td>58886.439456</td>\n",
       "      <td>175.776546</td>\n",
       "      <td>15.567160</td>\n",
       "      <td>1</td>\n",
       "      <td>733.608232</td>\n",
       "      <td>7.126595</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>ZTF18aaaqexr</td>\n",
       "      <td>58891.320255</td>\n",
       "      <td>175.776546</td>\n",
       "      <td>15.567160</td>\n",
       "      <td>2</td>\n",
       "      <td>1504.452801</td>\n",
       "      <td>10.895416</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>ZTF18aaaqexr</td>\n",
       "      <td>58891.420880</td>\n",
       "      <td>175.776546</td>\n",
       "      <td>15.567160</td>\n",
       "      <td>1</td>\n",
       "      <td>796.023260</td>\n",
       "      <td>10.651924</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>ZTF18aaaqexr</td>\n",
       "      <td>58894.353704</td>\n",
       "      <td>175.776546</td>\n",
       "      <td>15.567160</td>\n",
       "      <td>1</td>\n",
       "      <td>772.638964</td>\n",
       "      <td>4.406099</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ZTF18aaaqexr</td>\n",
       "      <td>58894.468345</td>\n",
       "      <td>175.776546</td>\n",
       "      <td>15.567160</td>\n",
       "      <td>2</td>\n",
       "      <td>1487.203851</td>\n",
       "      <td>4.759942</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917930</th>\n",
       "      <td>ZTF21abidtrd</td>\n",
       "      <td>59405.317500</td>\n",
       "      <td>283.249749</td>\n",
       "      <td>28.959674</td>\n",
       "      <td>1</td>\n",
       "      <td>506.245423</td>\n",
       "      <td>8.319171</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917931</th>\n",
       "      <td>ZTF21abidtrd</td>\n",
       "      <td>59407.281354</td>\n",
       "      <td>283.249749</td>\n",
       "      <td>28.959674</td>\n",
       "      <td>1</td>\n",
       "      <td>487.892594</td>\n",
       "      <td>8.267767</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917932</th>\n",
       "      <td>ZTF21abidtrd</td>\n",
       "      <td>59407.317176</td>\n",
       "      <td>283.249749</td>\n",
       "      <td>28.959674</td>\n",
       "      <td>2</td>\n",
       "      <td>870.134863</td>\n",
       "      <td>9.425640</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917933</th>\n",
       "      <td>ZTF21abidtrd</td>\n",
       "      <td>59409.296111</td>\n",
       "      <td>283.249749</td>\n",
       "      <td>28.959674</td>\n",
       "      <td>2</td>\n",
       "      <td>862.413520</td>\n",
       "      <td>9.234165</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917934</th>\n",
       "      <td>ZTF21abidtrd</td>\n",
       "      <td>59409.333646</td>\n",
       "      <td>283.249749</td>\n",
       "      <td>28.959674</td>\n",
       "      <td>1</td>\n",
       "      <td>479.596502</td>\n",
       "      <td>9.483954</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>493777 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 oid           mjd          ra        dec  fid  flux_tot_ujy  \\\n",
       "91      ZTF18aaaqexr  58886.439456  175.776546  15.567160    1    733.608232   \n",
       "92      ZTF18aaaqexr  58891.320255  175.776546  15.567160    2   1504.452801   \n",
       "93      ZTF18aaaqexr  58891.420880  175.776546  15.567160    1    796.023260   \n",
       "94      ZTF18aaaqexr  58894.353704  175.776546  15.567160    1    772.638964   \n",
       "95      ZTF18aaaqexr  58894.468345  175.776546  15.567160    2   1487.203851   \n",
       "...              ...           ...         ...        ...  ...           ...   \n",
       "917930  ZTF21abidtrd  59405.317500  283.249749  28.959674    1    506.245423   \n",
       "917931  ZTF21abidtrd  59407.281354  283.249749  28.959674    1    487.892594   \n",
       "917932  ZTF21abidtrd  59407.317176  283.249749  28.959674    2    870.134863   \n",
       "917933  ZTF21abidtrd  59409.296111  283.249749  28.959674    2    862.413520   \n",
       "917934  ZTF21abidtrd  59409.333646  283.249749  28.959674    1    479.596502   \n",
       "\n",
       "        fluxunc_tot_ujy_resc alerceclass  \n",
       "91                  7.126595        SNIa  \n",
       "92                 10.895416        SNIa  \n",
       "93                 10.651924        SNIa  \n",
       "94                  4.406099        SNIa  \n",
       "95                  4.759942        SNIa  \n",
       "...                      ...         ...  \n",
       "917930              8.319171        SNIa  \n",
       "917931              8.267767        SNIa  \n",
       "917932              9.425640        SNIa  \n",
       "917933              9.234165        SNIa  \n",
       "917934              9.483954        SNIa  \n",
       "\n",
       "[493777 rows x 8 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ztf_data = pd.read_pickle('data/ztf/lc_from_first_mjd_forced_fot.pkl')\n",
    "ztf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ztf_data.fid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bands = [1, 2]\n",
    "bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oid</th>\n",
       "      <th>mjd</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>fid</th>\n",
       "      <th>flux_tot_ujy</th>\n",
       "      <th>fluxunc_tot_ujy_resc</th>\n",
       "      <th>alerceclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>ZTF18aaaqexr</td>\n",
       "      <td>58886.439456</td>\n",
       "      <td>175.776546</td>\n",
       "      <td>15.567160</td>\n",
       "      <td>1</td>\n",
       "      <td>733.608232</td>\n",
       "      <td>7.126595</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>ZTF18aaaqexr</td>\n",
       "      <td>58891.320255</td>\n",
       "      <td>175.776546</td>\n",
       "      <td>15.567160</td>\n",
       "      <td>2</td>\n",
       "      <td>1504.452801</td>\n",
       "      <td>10.895416</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>ZTF18aaaqexr</td>\n",
       "      <td>58891.420880</td>\n",
       "      <td>175.776546</td>\n",
       "      <td>15.567160</td>\n",
       "      <td>1</td>\n",
       "      <td>796.023260</td>\n",
       "      <td>10.651924</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>ZTF18aaaqexr</td>\n",
       "      <td>58894.353704</td>\n",
       "      <td>175.776546</td>\n",
       "      <td>15.567160</td>\n",
       "      <td>1</td>\n",
       "      <td>772.638964</td>\n",
       "      <td>4.406099</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ZTF18aaaqexr</td>\n",
       "      <td>58894.468345</td>\n",
       "      <td>175.776546</td>\n",
       "      <td>15.567160</td>\n",
       "      <td>2</td>\n",
       "      <td>1487.203851</td>\n",
       "      <td>4.759942</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917930</th>\n",
       "      <td>ZTF21abidtrd</td>\n",
       "      <td>59405.317500</td>\n",
       "      <td>283.249749</td>\n",
       "      <td>28.959674</td>\n",
       "      <td>1</td>\n",
       "      <td>506.245423</td>\n",
       "      <td>8.319171</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917931</th>\n",
       "      <td>ZTF21abidtrd</td>\n",
       "      <td>59407.281354</td>\n",
       "      <td>283.249749</td>\n",
       "      <td>28.959674</td>\n",
       "      <td>1</td>\n",
       "      <td>487.892594</td>\n",
       "      <td>8.267767</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917932</th>\n",
       "      <td>ZTF21abidtrd</td>\n",
       "      <td>59407.317176</td>\n",
       "      <td>283.249749</td>\n",
       "      <td>28.959674</td>\n",
       "      <td>2</td>\n",
       "      <td>870.134863</td>\n",
       "      <td>9.425640</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917933</th>\n",
       "      <td>ZTF21abidtrd</td>\n",
       "      <td>59409.296111</td>\n",
       "      <td>283.249749</td>\n",
       "      <td>28.959674</td>\n",
       "      <td>2</td>\n",
       "      <td>862.413520</td>\n",
       "      <td>9.234165</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917934</th>\n",
       "      <td>ZTF21abidtrd</td>\n",
       "      <td>59409.333646</td>\n",
       "      <td>283.249749</td>\n",
       "      <td>28.959674</td>\n",
       "      <td>1</td>\n",
       "      <td>479.596502</td>\n",
       "      <td>9.483954</td>\n",
       "      <td>SNIa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>485965 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 oid           mjd          ra        dec  fid  flux_tot_ujy  \\\n",
       "91      ZTF18aaaqexr  58886.439456  175.776546  15.567160    1    733.608232   \n",
       "92      ZTF18aaaqexr  58891.320255  175.776546  15.567160    2   1504.452801   \n",
       "93      ZTF18aaaqexr  58891.420880  175.776546  15.567160    1    796.023260   \n",
       "94      ZTF18aaaqexr  58894.353704  175.776546  15.567160    1    772.638964   \n",
       "95      ZTF18aaaqexr  58894.468345  175.776546  15.567160    2   1487.203851   \n",
       "...              ...           ...         ...        ...  ...           ...   \n",
       "917930  ZTF21abidtrd  59405.317500  283.249749  28.959674    1    506.245423   \n",
       "917931  ZTF21abidtrd  59407.281354  283.249749  28.959674    1    487.892594   \n",
       "917932  ZTF21abidtrd  59407.317176  283.249749  28.959674    2    870.134863   \n",
       "917933  ZTF21abidtrd  59409.296111  283.249749  28.959674    2    862.413520   \n",
       "917934  ZTF21abidtrd  59409.333646  283.249749  28.959674    1    479.596502   \n",
       "\n",
       "        fluxunc_tot_ujy_resc alerceclass  \n",
       "91                  7.126595        SNIa  \n",
       "92                 10.895416        SNIa  \n",
       "93                 10.651924        SNIa  \n",
       "94                  4.406099        SNIa  \n",
       "95                  4.759942        SNIa  \n",
       "...                      ...         ...  \n",
       "917930              8.319171        SNIa  \n",
       "917931              8.267767        SNIa  \n",
       "917932              9.425640        SNIa  \n",
       "917933              9.234165        SNIa  \n",
       "917934              9.483954        SNIa  \n",
       "\n",
       "[485965 rows x 8 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ztf_data[ztf_data.fid.isin(bands)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ztf_data[ztf_data.fid != 'i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g', 'r']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bands_name = ['g', 'r']\n",
    "bands_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False, True]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = [True, False, True]\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot = []\n",
    "tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot.append([])\n",
    "tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], []]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot.append([])\n",
    "tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tot[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "t = []\n",
    "g_temp_arr = []\n",
    "r_temp_arr = []\n",
    "g_err_temp_arr = []\n",
    "r_err_temp_arr = []\n",
    "\n",
    "for i in range(6):\n",
    "    t.append(random.randint(1,50))\n",
    "    g_temp_arr.append(random.randint(1,50))\n",
    "    r_temp_arr.append(random.randint(1,50))\n",
    "    g_err_temp_arr.append(random.randint(1,50))\n",
    "    r_err_temp_arr.append(random.randint(1,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: [34, 37, 44, 22, 33, 15]\n",
      "g_temp_arr: [4, 37, 47, 2, 15, 50]\n",
      "r_temp_arr: [43, 24, 16, 37, 50, 21]\n",
      "g_err_temp_arr: [20, 15, 28, 10, 14, 38]\n",
      "r_err_temp_arr: [15, 29, 4, 45, 33, 8]\n"
     ]
    }
   ],
   "source": [
    "print(f\"t: {t}\")\n",
    "print(f\"g_temp_arr: {g_temp_arr}\")\n",
    "print(f\"r_temp_arr: {r_temp_arr}\")\n",
    "print(f\"g_err_temp_arr: {g_err_temp_arr}\")\n",
    "print(f\"r_err_temp_arr: {r_err_temp_arr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34, 37, 44, 22, 33, 15]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "band_arr: [[4, 37, 47, 2, 15, 50], [43, 24, 16, 37, 50, 21]]\n",
      "band_arr_err: [[20, 15, 28, 10, 14, 38], [15, 29, 4, 45, 33, 8]]\n"
     ]
    }
   ],
   "source": [
    "band_arr = [g_temp_arr]+ [r_temp_arr]\n",
    "band_arr_err = [g_err_temp_arr]+ [r_err_temp_arr]\n",
    "\n",
    "print(f\"band_arr: {band_arr}\")\n",
    "print(f\"band_arr_err: {band_arr_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[34, 4, 20, 43, 15],\n",
       " [37, 37, 15, 24, 29],\n",
       " [44, 47, 28, 16, 4],\n",
       " [22, 2, 10, 37, 45],\n",
       " [33, 15, 14, 50, 33],\n",
       " [15, 50, 38, 21, 8]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = []\n",
    "for i in range(len(t)):\n",
    "    aux = []\n",
    "    aux.append(t[i])\n",
    "    for idx_band in range(2):\n",
    "        aux.append(band_arr[idx_band][i])\n",
    "        aux.append(band_arr_err[idx_band][i])\n",
    "    obs.append(aux)\n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[34, 4, 43, 20, 15],\n",
       " [37, 37, 24, 15, 29],\n",
       " [44, 47, 16, 28, 4],\n",
       " [22, 2, 37, 10, 45],\n",
       " [33, 15, 50, 14, 33],\n",
       " [15, 50, 21, 38, 8]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = [[t[i], g_temp_arr[i], r_temp_arr[i], g_err_temp_arr[i], r_err_temp_arr[i]] for i in range(len(t))]\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "t = []\n",
    "band_temp_arr = []\n",
    "band_err_temp_arr = []\n",
    "\n",
    "for idx_band in range(2):\n",
    "    band_temp_arr.append([])\n",
    "    band_err_temp_arr.append([])\n",
    "\n",
    "\n",
    "for j in range(2):\n",
    "    for i in range(6):\n",
    "        if j == 0: t.append(random.randint(1,50))\n",
    "        band_temp_arr[j].append(random.randint(1,50))\n",
    "        band_err_temp_arr[j].append(random.randint(1,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: [9, 1, 45, 49, 5, 46]\n",
      "band_temp_arr: [[38, 43, 30, 8, 37, 13], [17, 42, 14, 46, 37, 12]]\n",
      "band_err_temp_arr: [[11, 45, 12, 1, 35, 38], [17, 9, 2, 36, 21, 6]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"t: {t}\")\n",
    "print(f\"band_temp_arr: {band_temp_arr}\")\n",
    "print(f\"band_err_temp_arr: {band_err_temp_arr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9, 1, 45, 49, 5, 46],\n",
       " [38, 43, 30, 8, 37, 13],\n",
       " [17, 42, 14, 46, 37, 12],\n",
       " [11, 45, 12, 1, 35, 38],\n",
       " [17, 9, 2, 36, 21, 6]]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = [t] + band_temp_arr + band_err_temp_arr\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[38, 43, 30, 8, 37, 13],\n",
       " [17, 42, 14, 46, 37, 12],\n",
       " [11, 45, 12, 1, 35, 38],\n",
       " [17, 9, 2, 36, 21, 6]]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[38, 43, 30, 8, 37, 13],\n",
       " [17, 42, 14, 46, 37, 12],\n",
       " [11, 45, 12, 1, 35, 38],\n",
       " [17, 9, 2, 36, 21, 6]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[1:len(obs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[38, 43, 30, 8, 37, 13],\n",
       " [17, 42, 14, 46, 37, 12],\n",
       " [11, 45, 12, 1, 35, 38],\n",
       " [17, 9, 2, 36, 21, 6]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = len(obs)\n",
    "obs[1:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def prueba(lista):\n",
    "    lista.append(3)\n",
    "    lista.append(4)\n",
    "    lista_1 = lista.copy()\n",
    "    lista_1 = lista_1 + [34]\n",
    "\n",
    "    return lista_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 34]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_prueba = []\n",
    "ojito = prueba(lista_prueba)\n",
    "ojito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "tot = []\n",
    "tot.append(math.prod(lista))\n",
    "tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all(tot):\n",
    "    print(\"HOLY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call (Temp/ipykernel_16184/1476790445.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\damor\\AppData\\Local\\Temp/ipykernel_16184/1476790445.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    np.zeros(len(bands)) = -999\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m cannot assign to function call\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.zeros(len(bands)) = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-999, -999])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.full(len(bands), -999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = r = -999 \n",
    "g_error = r_error = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5, 9, 67, 9], [7, 87, 4, 327, 2], [3, 6, 8, 654, 4]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = [[1,5,9,67,9], [7,87,4,327,2], [3,6,8,654,4]]\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 7, 3]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[obs[i][0] for i in range(len(obs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-999., -999.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bands = np.full((2), -999, dtype=float)\n",
    "bands_err = np.full((2), -999, dtype=float)\n",
    "bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 87, 6], [9, 4, 8]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bands_arr = []\n",
    "bands_error_arr = []\n",
    "\n",
    "for idx_band in range(2):\n",
    "    bands_arr.append([obs[i][idx_band+1] for i in range(len(obs))])\n",
    "    bands_error_arr.append([obs[i][idx_band+1+len(bands_name)] for i in range(len(obs))])\n",
    "\n",
    "bands_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[67, 327, 654], [9, 2, 4]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bands_error_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.LogSoftmax(dim=1)\n",
    "loss = torch.nn.NLLLoss()\n",
    "# input is of size N x C = 3 x 5\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "output = loss(m(input), target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D loss example (used, for example, with image inputs)\n",
    "N, C = 5, 4\n",
    "loss = torch.nn.NLLLoss()\n",
    "# input is of size N x C x height x width\n",
    "data = torch.randn(N, 16, 10, 10)\n",
    "conv = torch.nn.Conv2d(16, C, (3, 3))\n",
    "m = torch.nn.LogSoftmax(dim=1)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)\n",
    "output = loss(m(conv(data)), target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 8, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(conv(data)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5050, -1.6025, -1.9498, -1.4743, -1.5831],\n",
       "        [-1.5557, -1.9238, -1.1835, -1.7494, -1.8152],\n",
       "        [-1.5943, -1.7358, -1.9226, -1.8609, -1.1428]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.log_softmax(torch.rand(3,5, requires_grad=True), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1], device='cuda:0'),\n",
       " tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0], device='cuda:0'),\n",
       " tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1], device='cuda:0')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "lista = [torch.tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1]).to('cuda'), torch.tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0]).to('cuda'), torch.tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1]).to('cuda')]\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((lista),dim=0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "#def collate_fn(data):\n",
    "#    data.sort(key=lambda x: len(x), reverse=True)\n",
    "#    data = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "#    return data\n",
    "\n",
    "def collate_fn(data):\n",
    "    data.sort(key=lambda x: len(x), reverse=True)\n",
    "    seq_len = [s.size(0) for s in data]\n",
    "    \n",
    "    data = pad_sequence(data, batch_first=True).float() \n",
    "    print(f\"data: {data}\")\n",
    "\n",
    "    data = data.unsqueeze(-1)\n",
    "    print(f\"data unsqueeze: {data}\")\n",
    "\n",
    "    data = pack_padded_sequence(data, seq_len, batch_first=True)\n",
    "    return data\n",
    "\n",
    "a = torch.tensor([1,2,3,4])\n",
    "b = torch.tensor([5,6,7])\n",
    "c = torch.tensor([7,8])\n",
    "d = torch.tensor([9])\n",
    "train_x = [a, b, c, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MyData(train_x)\n",
    "data_loader = DataLoader(data, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: tensor([[1., 2., 3., 4.],\n",
      "        [9., 0., 0., 0.]])\n",
      "data unsqueeze: tensor([[[1.],\n",
      "         [2.],\n",
      "         [3.],\n",
      "         [4.]],\n",
      "\n",
      "        [[9.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[1.],\n",
       "        [9.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.]]), batch_sizes=tensor([2, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x = iter(data_loader).next()\n",
    "batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [9.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(batch_x.data.size())\n",
    "batch_x.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(1, 4, batch_first=True)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.RNN(input_size=1, hidden_size=4, num_layers=1, batch_first=True)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7152, 0.1443, 0.1762, 0.5152],\n",
       "         [0.6478, 0.8451, 0.1993, 0.9277]]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0 = torch.rand(1, 2, 4).float()\n",
    "print(h0.size())\n",
    "h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-0.1387,  0.8321, -0.3218,  0.9018],\n",
       "        [ 0.8790,  0.9997, -0.2100,  0.9993],\n",
       "        [ 0.4744,  0.8615, -0.3654,  0.9228],\n",
       "        [ 0.4153,  0.9606, -0.3229,  0.9731],\n",
       "        [ 0.5730,  0.9813, -0.3105,  0.9845]], grad_fn=<CatBackward0>), batch_sizes=tensor([2, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, h1 = rnn(batch_x, h0)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1387,  0.8321, -0.3218,  0.9018],\n",
       "         [ 0.4744,  0.8615, -0.3654,  0.9228],\n",
       "         [ 0.4153,  0.9606, -0.3229,  0.9731],\n",
       "         [ 0.5730,  0.9813, -0.3105,  0.9845]],\n",
       "\n",
       "        [[ 0.8790,  0.9997, -0.2100,  0.9993],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_pad, out_len = pad_packed_sequence(out, batch_first=True)\n",
    "out_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 1])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3377,  0.6104, -0.2931,  0.4540, -0.3063,  0.9195,  0.8094,  0.0117],\n",
       "        [-0.5594,  1.6916,  0.8256, -1.5459,  1.3003,  0.1594, -0.0759,  0.5071],\n",
       "        [-0.3560,  0.8762,  0.1942, -0.2805, -0.0241, -1.4893, -0.6983, -1.4939],\n",
       "        [-0.4580, -1.1823,  0.4549, -0.0122, -0.0450, -0.7694,  0.6710,  0.0550],\n",
       "        [-0.9885, -0.5520,  1.7361, -0.8071,  0.6708, -0.6326,  0.1010, -0.9185],\n",
       "        [-0.4420, -0.1469, -0.4238, -0.8563, -0.1200,  1.7409, -0.2473,  1.3851]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_large = torch.randn(6, 8)\n",
    "vector_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.7512e-01, -1.5158e-01, -6.7957e-01, -9.4990e-01,  2.5984e+00,\n",
       "          9.2269e-01, -9.6934e-01,  1.4508e+00],\n",
       "        [-1.0017e+00,  4.9982e-02, -2.0450e+00,  1.4372e-01, -5.7959e-01,\n",
       "         -9.9337e-01,  1.5771e-01,  2.7913e-01],\n",
       "        [-1.7352e-03, -4.6116e-01,  2.3124e-01,  1.5909e+00, -1.2297e+00,\n",
       "         -5.5109e-02,  1.4122e+00, -1.4810e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_normal = torch.randn(3, 8)\n",
    "vector_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_large.shape[0]-vector_normal.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  9.7512e-01, -1.5158e-01,\n",
       "         -6.7957e-01, -9.4990e-01,  2.5984e+00,  9.2269e-01, -9.6934e-01,\n",
       "          1.4508e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0017e+00,  4.9982e-02,\n",
       "         -2.0450e+00,  1.4372e-01, -5.7959e-01, -9.9337e-01,  1.5771e-01,\n",
       "          2.7913e-01],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.7352e-03, -4.6116e-01,\n",
       "          2.3124e-01,  1.5909e+00, -1.2297e+00, -5.5109e-02,  1.4122e+00,\n",
       "         -1.4810e+00]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.pad(vector_normal, (vector_large.shape[0]-vector_normal.shape[0], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  9.7512e-01, -1.5158e-01,\n",
       "         -6.7957e-01, -9.4990e-01,  2.5984e+00,  9.2269e-01, -9.6934e-01,\n",
       "          1.4508e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0017e+00,  4.9982e-02,\n",
       "         -2.0450e+00,  1.4372e-01, -5.7959e-01, -9.9337e-01,  1.5771e-01,\n",
       "          2.7913e-01],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00, -1.7352e-03, -4.6116e-01,\n",
       "          2.3124e-01,  1.5909e+00, -1.2297e+00, -5.5109e-02,  1.4122e+00,\n",
       "         -1.4810e+00]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.pad(vector_normal, (8, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3377e+00,  6.1037e-01, -2.9312e-01,  4.5401e-01, -3.0630e-01,\n",
       "           9.1946e-01,  8.0943e-01,  1.1676e-02],\n",
       "         [-5.5942e-01,  1.6916e+00,  8.2563e-01, -1.5459e+00,  1.3003e+00,\n",
       "           1.5939e-01, -7.5907e-02,  5.0711e-01],\n",
       "         [-3.5603e-01,  8.7622e-01,  1.9418e-01, -2.8051e-01, -2.4069e-02,\n",
       "          -1.4893e+00, -6.9832e-01, -1.4939e+00],\n",
       "         [-4.5797e-01, -1.1823e+00,  4.5491e-01, -1.2174e-02, -4.5019e-02,\n",
       "          -7.6943e-01,  6.7103e-01,  5.5011e-02],\n",
       "         [-9.8851e-01, -5.5203e-01,  1.7361e+00, -8.0710e-01,  6.7082e-01,\n",
       "          -6.3264e-01,  1.0104e-01, -9.1846e-01],\n",
       "         [-4.4203e-01, -1.4694e-01, -4.2381e-01, -8.5629e-01, -1.1998e-01,\n",
       "           1.7409e+00, -2.4732e-01,  1.3851e+00]],\n",
       "\n",
       "        [[ 9.7512e-01, -1.5158e-01, -6.7957e-01, -9.4990e-01,  2.5984e+00,\n",
       "           9.2269e-01, -9.6934e-01,  1.4508e+00],\n",
       "         [-1.0017e+00,  4.9982e-02, -2.0450e+00,  1.4372e-01, -5.7959e-01,\n",
       "          -9.9337e-01,  1.5771e-01,  2.7913e-01],\n",
       "         [-1.7352e-03, -4.6116e-01,  2.3124e-01,  1.5909e+00, -1.2297e+00,\n",
       "          -5.5109e-02,  1.4122e+00, -1.4810e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.nn.utils.rnn.pad_sequence([vector_large, vector_normal], batch_first=True, padding_value=0.0)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 8])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_large.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_large.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([-1.0017,  0.9751,  0.0500, -0.1516, -2.0450, -0.6796,  0.1437, -0.9499,\n",
       "        -0.5796,  2.5984, -0.9934,  0.9227,  0.1577,  0.2791]), batch_sizes=tensor([2, 2, 2, 2, 2, 2, 1, 1]), sorted_indices=tensor([1, 0]), unsorted_indices=tensor([1, 0]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_packed = torch.nn.utils.rnn.pack_padded_sequence(vector_normal, vector_large.shape, batch_first=True, enforce_sorted=False)\n",
    "x_packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0032, -0.9792,  1.1070,  1.0795],\n",
       "        [ 0.3225,  0.4289, -1.3852, -0.2841],\n",
       "        [ 1.0932, -0.8647,  0.8619, -0.2636]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[0, 0, 0, 0]])\n",
    "\n",
    "padding = torch.cat((a, x), 0)\n",
    "padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True],\n",
       "        [False, False,  True,  True],\n",
       "        [ True,  True, False, False],\n",
       "        [ True, False,  True, False]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = padding.ge(0.0)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pack = torch.nn.utils.rnn.pack_padded_sequence(batch_in, seq_lengths, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "batch_size = 3\n",
    "max_length = 3\n",
    "hidden_size = 2\n",
    "n_layers = 1\n",
    "\n",
    "# container\n",
    "batch_in = torch.zeros((batch_size, 1, max_length))\n",
    "batch_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2., 3.]],\n",
       "\n",
       "        [[1., 2., 0.]],\n",
       "\n",
       "        [[1., 0., 0.]]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data\n",
    "vec_1 = torch.FloatTensor([[1, 2, 3]])\n",
    "vec_2 = torch.FloatTensor([[1, 2, 0]])\n",
    "vec_3 = torch.FloatTensor([[1, 0, 0]])\n",
    "\n",
    "batch_in[0] = vec_1\n",
    "batch_in[1] = vec_2\n",
    "batch_in[2] = vec_3\n",
    "batch_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2., 3.]],\n",
       "\n",
       "        [[1., 2., 0.]],\n",
       "\n",
       "        [[1., 0., 0.]]], requires_grad=True)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_in.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 3])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_in.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths = [3,2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[1., 2., 3.],\n",
       "        [1., 2., 0.],\n",
       "        [1., 0., 0.]], grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([3, 2, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pack = torch.nn.utils.rnn.pack_padded_sequence(batch_in, seq_lengths, batch_first=True)\n",
    "pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(3, 2, batch_first=True)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize\n",
    "## (batch_size=N, sequence_length=L, input_size=H_in)\n",
    "# rnn = nn.RNN(max_length, hidden_size, n_layers, batch_first=True) \n",
    "rnn = nn.RNN(input_size=batch_in.size(0), hidden_size=hidden_size, num_layers=n_layers, batch_first=True) \n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4801, -0.0870],\n",
       "         [ 0.2476, -0.2566],\n",
       "         [ 0.2596, -2.1887]]], requires_grad=True)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Dâˆ—num_layers, N, H_out)\n",
    "h0 = torch.randn((n_layers, batch_size, hidden_size), requires_grad=True)\n",
    "h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[1., 2., 3.],\n",
       "        [1., 2., 0.],\n",
       "        [1., 0., 0.]], grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([3, 2, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [1., 2., 0.],\n",
       "        [1., 0., 0.]], grad_fn=<PackPaddedSequenceBackward0>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pack.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [1., 2., 0.],\n",
       "        [1., 0., 0.]], grad_fn=<PackPaddedSequenceBackward0>)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pack.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "start (3) + length (2) exceeds dimension size (3).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16936\\913292072.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\SSL-project\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\SSL-project\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    269\u001b[0m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    270\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0m\u001b[0;32m    272\u001b[0m                            self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: start (3) + length (2) exceeds dimension size (3)."
     ]
    }
   ],
   "source": [
    "#forward \n",
    "out, _ = rnn(pack, h0)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(10, 20, num_layers=2)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.RNN(10, 20, 2)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[1.],\n",
       "        [4.],\n",
       "        [6.],\n",
       "        [2.],\n",
       "        [5.],\n",
       "        [3.]]), batch_sizes=tensor([3, 2, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([[1], [2], [3]])\n",
    "b = torch.Tensor([[4], [5]])\n",
    "c = torch.Tensor([[6]])\n",
    "packed = torch.nn.utils.rnn.pack_sequence([a, b, c])\n",
    "packed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(1, 3)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = nn.LSTM(1,3)\n",
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-0.0659, -0.0926, -0.1677],\n",
       "        [-0.1125, -0.0323,  0.2522],\n",
       "        [-0.0634, -0.0132,  0.4770],\n",
       "        [-0.1451, -0.1283, -0.2020],\n",
       "        [-0.1288, -0.0313,  0.5098],\n",
       "        [-0.1826, -0.1290, -0.0847]], grad_fn=<CatBackward0>), batch_sizes=tensor([3, 2, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_output, (h,c) = lstm(packed)\n",
    "packed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1826, -0.1290, -0.0847],\n",
       "          [-0.1288, -0.0313,  0.5098],\n",
       "          [-0.0634, -0.0132,  0.4770]]], grad_fn=<StackBackward0>),\n",
       " tensor([[[-0.8498, -0.3826, -0.1405],\n",
       "          [-1.3391, -0.2029,  0.9316],\n",
       "          [-0.8320, -0.1094,  0.7377]]], grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0659, -0.0926, -0.1677],\n",
       "          [-0.1125, -0.0323,  0.2522],\n",
       "          [-0.0634, -0.0132,  0.4770]],\n",
       " \n",
       "         [[-0.1451, -0.1283, -0.2020],\n",
       "          [-0.1288, -0.0313,  0.5098],\n",
       "          [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.1826, -0.1290, -0.0847],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]], grad_fn=<CopySlices>),\n",
       " tensor([3, 2, 1]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "start (3) + length (2) exceeds dimension size (3).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16936\\913292072.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\SSL-project\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\SSL-project\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    269\u001b[0m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    270\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0m\u001b[0;32m    272\u001b[0m                            self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: start (3) + length (2) exceeds dimension size (3)."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "start (3) + length (2) exceeds dimension size (3).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16936\\913292072.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\SSL-project\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\SSL-project\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    269\u001b[0m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    270\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0m\u001b[0;32m    272\u001b[0m                            self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: start (3) + length (2) exceeds dimension size (3)."
     ]
    }
   ],
   "source": [
    "#forward \n",
    "out, _ = rnn(pack, h0)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 3\n",
    "# max_length = 3\n",
    "# hidden_size = 2\n",
    "# n_layers =1\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'batch_sizes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16936\\272390304.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# unpack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0munpacked\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munpacked_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0munpacked\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\SSL-project\\lib\\site-packages\\torch\\nn\\utils\\rnn.py\u001b[0m in \u001b[0;36mpad_packed_sequence\u001b[1;34m(sequence, batch_first, padding_value, total_length)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m     \"\"\"\n\u001b[1;32m--> 306\u001b[1;33m     \u001b[0mmax_seq_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtotal_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtotal_length\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'batch_sizes'"
     ]
    }
   ],
   "source": [
    "# unpack\n",
    "unpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(out)\n",
    "unpacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unpacked_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16936\\3054666514.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0munpacked_len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'unpacked_len' is not defined"
     ]
    }
   ],
   "source": [
    "unpacked_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "max_length = 3\n",
    "hidden_size = 2\n",
    "n_layers =1\n",
    "feature_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# container\n",
    "#batch_in = torch.zeros((batch_size, max_length, feature_dim)) # 2\n",
    "batch_in = torch.zeros((batch_size, 1, max_length)) # 1\n",
    "batch_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2., 3.]],\n",
       "\n",
       "        [[1., 2., 0.]],\n",
       "\n",
       "        [[1., 0., 0.]],\n",
       "\n",
       "        [[2., 0., 0.]]], requires_grad=True)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data\n",
    "vec_1 = torch.FloatTensor([[1, 2, 3]])\n",
    "vec_2 = torch.FloatTensor([[1, 2, 0]])\n",
    "vec_3 = torch.FloatTensor([[1, 0, 0]])\n",
    "vec_4 = torch.FloatTensor([[2, 0, 0]])\n",
    "\n",
    "batch_in[0] = vec_1\n",
    "batch_in[1] = vec_2\n",
    "batch_in[2] = vec_3\n",
    "batch_in[3] = vec_4\n",
    "\n",
    "#batch_in = Variable(batch_in)\n",
    "batch_in.requires_grad_()\n",
    "batch_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(batch_in.size())\n",
    "seq_lengths = [3,2,1,1] # list of integers holding information about the batch size at each sequence step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([[1., 2., 3.],\n",
      "        [1., 2., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [2., 0., 0.]], grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([4, 2, 1]), sorted_indices=None, unsorted_indices=None)\n"
     ]
    }
   ],
   "source": [
    "# pack it\n",
    "pack = torch.nn.utils.rnn.pack_padded_sequence(batch_in, seq_lengths, batch_first=True)\n",
    "print(pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(4, 2, batch_first=True)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize\n",
    "rnn = nn.RNN(batch_in.size(0), hidden_size, n_layers, batch_first=True)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7133, -1.1805],\n",
       "         [-0.7070,  0.4656],\n",
       "         [ 0.3746,  0.6887],\n",
       "         [-0.8825, -0.5778]]], requires_grad=True)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0 = torch.randn((n_layers, batch_size, hidden_size), requires_grad=True)\n",
    "h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 4, got 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16936\\913292072.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\SSL-project\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\SSL-project\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mhx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[0m_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_rnn_impls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\SSL-project\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\SSL-project\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    203\u001b[0m                     expected_input_dim, input.dim()))\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             raise RuntimeError(\n\u001b[0m\u001b[0;32m    206\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m    207\u001b[0m                     self.input_size, input.size(-1)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 4, got 3"
     ]
    }
   ],
   "source": [
    "#forward \n",
    "out, _ = rnn(pack, h0)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# unpack\n",
    "unpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "print(unpacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1.],\n",
       "         [2.],\n",
       "         [3.]],\n",
       "\n",
       "        [[1.],\n",
       "         [2.],\n",
       "         [0.]],\n",
       "\n",
       "        [[1.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[2.],\n",
       "         [0.],\n",
       "         [0.]]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "max_length = 3\n",
    "hidden_size = 2\n",
    "n_layers =1\n",
    "feature_dim = 1\n",
    "\n",
    "# container\n",
    "batch_in = torch.zeros((batch_size, max_length, feature_dim))\n",
    "\n",
    "# data\n",
    "vec_1 = torch.FloatTensor([[[1], [2], [3]]])\n",
    "vec_2 = torch.FloatTensor([[[1], [2], [0]]])\n",
    "vec_3 = torch.FloatTensor([[[1], [0], [0]]])\n",
    "vec_4 = torch.FloatTensor([[[2], [0], [0]]])\n",
    "\n",
    "batch_in[0] = vec_1\n",
    "batch_in[1] = vec_2\n",
    "batch_in[2] = vec_3\n",
    "batch_in[3] = vec_4\n",
    "print(batch_in.size())\n",
    "batch_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [3.]]), batch_sizes=tensor([4, 2, 1]), sorted_indices=None, unsorted_indices=None)\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = [3,2,1,1] # list of integers holding information about the batch size at each sequence step\n",
    "\n",
    "# pack it\n",
    "pack = torch.nn.utils.rnn.pack_padded_sequence(batch_in, seq_lengths, batch_first=True)\n",
    "print(pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pack.data.size()\n",
    "pack.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(feature_dim, hidden_size, n_layers, batch_first=True) \n",
    "h0 = Variable(torch.randn(n_layers, batch_size, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward \n",
    "out, _ = rnn(pack, h0)\n",
    "\n",
    "# unpack\n",
    "unpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "print(unpacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,  9,  9,  9,  9,  9,  9,  9,\n",
    "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  8,  7,  7,  6,  5,  5,  5,  4,\n",
    "         4,  4,  4,  3,  2,  2,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
    "         1,  1,  1,  1,  1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[61],\n",
       "        [40],\n",
       "        [39],\n",
       "        [39],\n",
       "        [37],\n",
       "        [33],\n",
       "        [28],\n",
       "        [18],\n",
       "        [17],\n",
       "        [14]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.tensor([61, 40, 39, 39, 37, 33, 28, 18, 17, 14]).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked = torch.tensor([[[ 1.6538e-02,  5.6767e-02,  9.0571e-04,  3.0647e-02,\n",
    "          -4.7211e-02,  7.6125e-02],\n",
    "         [ 1.3896e-02,  1.6257e-02, -1.6220e-02, -1.2280e-02,\n",
    "           8.3733e-03, -2.7838e-02],\n",
    "         [ 2.5685e-02,  3.3586e-02,  1.8123e-02,  2.4158e-02,\n",
    "           9.5614e-03,  3.1805e-02],\n",
    "         [-2.0610e-01, -7.0265e-02,  5.8711e-02, -4.2112e-03,\n",
    "           1.3706e-01, -3.6016e-02],\n",
    "         [-1.4957e-01,  1.4763e-02,  5.0862e-02,  2.8549e-02,\n",
    "           1.6792e-01,  3.1487e-02],\n",
    "         [-2.3777e-01, -5.7890e-02,  8.9409e-02,  5.7700e-03,\n",
    "           1.8906e-01,  4.9379e-02],\n",
    "         [-1.4957e-01,  1.4763e-02,  5.0862e-02,  2.8549e-02,\n",
    "           1.6792e-01,  3.1487e-02],\n",
    "         [-2.3777e-01, -5.7890e-02,  8.9409e-02,  5.7700e-03,\n",
    "           1.8906e-01,  4.9379e-02]],\n",
    "\n",
    "        [[-5.6594e-02,  1.6429e-02,  5.9902e-02,  3.7806e-02,\n",
    "           4.3941e-02,  9.6136e-02],\n",
    "         [-9.4041e-02, -1.6179e-02,  6.4783e-02, -3.0855e-02,\n",
    "           1.5764e-01, -2.1854e-02],\n",
    "         [-6.4557e-02, -6.2401e-02,  4.2734e-03,  1.1267e-01,\n",
    "           2.3573e-01,  6.0810e-02],\n",
    "         [-1.4957e-01,  1.4763e-02,  5.0862e-02,  2.8549e-02,\n",
    "           1.6792e-01,  3.1487e-02],\n",
    "         [-2.3777e-01, -5.7890e-02,  8.9409e-02,  5.7700e-03,\n",
    "           1.8906e-01,  4.9379e-02],           \n",
    "         [-1.4957e-01,  1.4763e-02,  5.0862e-02,  2.8549e-02,\n",
    "           1.6792e-01,  3.1487e-02],\n",
    "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
    "          -1.0000e+00, -1.0000e+00],\n",
    "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
    "          -1.0000e+00, -1.0000e+00]],\n",
    "\n",
    "        [[ 3.0731e-02,  3.4918e-02, -2.5661e-02,  7.8089e-02,\n",
    "          -1.3263e-01,  8.3883e-02],\n",
    "         [-4.1114e-02, -4.4349e-02, -4.6148e-02,  1.4719e-01,\n",
    "          -1.7767e-01,  1.1240e-01],\n",
    "         [-4.8966e-02, -3.3305e-02, -1.9329e-02,  1.5953e-01,\n",
    "           6.3815e-03,  1.4913e-01],\n",
    "         [-1.4957e-01,  1.4763e-02,  5.0862e-02,  2.8549e-02,\n",
    "           1.6792e-01,  3.1487e-02],\n",
    "         [-2.3777e-01, -5.7890e-02,  8.9409e-02,  5.7700e-03,\n",
    "           1.8906e-01,  4.9379e-02],\n",
    "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
    "          -1.0000e+00, -1.0000e+00],\n",
    "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
    "          -1.0000e+00, -1.0000e+00],\n",
    "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
    "          -1.0000e+00, -1.0000e+00]],\n",
    "\n",
    "\n",
    "        [[ 2.6054e-02, -1.0205e-02,  4.3393e-02, -3.8335e-02,\n",
    "          -1.1724e-02, -2.7068e-02],\n",
    "         [ 3.2665e-02, -1.0081e-02,  1.0885e-01,  1.2793e-01,\n",
    "           1.4153e-01,  4.8708e-02],\n",
    "         [ 5.3791e-02, -3.3659e-02,  1.2479e-01,  1.3702e-01,\n",
    "           1.4016e-01,  3.7281e-02],\n",
    "         [-1.4957e-01,  1.4763e-02,  5.0862e-02,  2.8549e-02,\n",
    "           1.6792e-01,  3.1487e-02],\n",
    "         [-2.3777e-01, -5.7890e-02,  8.9409e-02,  5.7700e-03,\n",
    "           1.8906e-01,  4.9379e-02],\n",
    "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
    "          -1.0000e+00, -1.0000e+00],\n",
    "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
    "          -1.0000e+00, -1.0000e+00],\n",
    "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
    "          -1.0000e+00, -1.0000e+00]],\n",
    "\n",
    "        [[ 3.5541e-03, -3.1010e-02, -4.0462e-02,  6.6716e-02,\n",
    "          -6.9438e-02,  4.3141e-02],\n",
    "         [ 1.4525e-02, -7.0359e-03, -3.1221e-03,  4.9759e-02,\n",
    "          -1.3915e-02,  9.0185e-02],\n",
    "         [ 5.9673e-04, -7.5668e-02, -1.3084e-04,  2.7947e-02,\n",
    "           8.1181e-02,  3.9457e-02],\n",
    "         [-1.4957e-01,  1.4763e-02,  5.0862e-02,  2.8549e-02,\n",
    "           1.6792e-01,  3.1487e-02],\n",
    "         [-2.3777e-01, -5.7890e-02,  8.9409e-02,  5.7700e-03,\n",
    "           1.8906e-01,  4.9379e-02],\n",
    "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
    "          -1.0000e+00, -1.0000e+00],\n",
    "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
    "          -1.0000e+00, -1.0000e+00],\n",
    "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
    "          -1.0000e+00, -1.0000e+00]],\n",
    "\n",
    "        [[-9.3490e-02, -3.7732e-02,  2.2119e-02,  8.1481e-02,\n",
    "          -2.6966e-02,  5.9264e-02],\n",
    "         [-1.8529e-01, -5.4526e-02,  3.7262e-02,  4.2626e-02,\n",
    "           1.5307e-01,  9.5252e-02],\n",
    "         [-2.7138e-01, -7.4981e-02,  1.0714e-01, -3.7837e-02,\n",
    "           1.7267e-01,  1.5304e-01],\n",
    "         [-1.4957e-01,  1.4763e-02,  5.0862e-02,  2.8549e-02,\n",
    "           1.6792e-01,  3.1487e-02],\n",
    "         [-2.3777e-01, -5.7890e-02,  8.9409e-02,  5.7700e-03,\n",
    "           1.8906e-01,  4.9379e-02],\n",
    "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
    "          -1.0000e+00, -1.0000e+00],\n",
    "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
    "          -1.0000e+00, -1.0000e+00],\n",
    "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
    "          -1.0000e+00, -1.0000e+00]],\n",
    "          \n",
    "         [[-9.3490e-02, -3.7732e-02,  2.2119e-02,  8.1481e-02,\n",
    "          -2.6966e-02,  5.9264e-02],\n",
    "         [-1.8529e-01, -5.4526e-02,  3.7262e-02,  4.2626e-02,\n",
    "           1.5307e-01,  9.5252e-02],\n",
    "         [-2.7138e-01, -7.4981e-02,  1.0714e-01, -3.7837e-02,\n",
    "           1.7267e-01,  1.5304e-01],\n",
    "         [-1.4957e-01,  1.4763e-02,  5.0862e-02,  2.8549e-02,\n",
    "           1.6792e-01,  3.1487e-02],\n",
    "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
    "          -1.0000e+00, -1.0000e+00],\n",
    "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
    "          -1.0000e+00, -1.0000e+00],\n",
    "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
    "          -1.0000e+00, -1.0000e+00],\n",
    "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
    "          -1.0000e+00, -1.0000e+00]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 8, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpacked.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 6, 5, 5, 5, 5, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpacked_out_len = torch.tensor([8,6,5,5,5,5,4])\n",
    "unpacked_out_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9247, -0.0499,  0.3421,  0.1070,  0.8217,  0.2058],\n",
       "        [-3.6025, -3.1053, -2.7308, -2.8461, -2.2057, -2.7840],\n",
       "        [-3.4467, -3.0859, -2.9509, -2.5809, -2.9469, -2.5737],\n",
       "        [-3.2748, -3.0971, -2.5827, -2.7391, -2.3731, -2.8602],\n",
       "        [-3.3687, -3.1568, -2.9034, -2.8213, -2.6452, -2.7464],\n",
       "        [-3.9375, -3.2104, -2.6932, -2.8794, -2.3442, -2.6116],\n",
       "        [-3.9375, -3.2104, -2.6932, -2.8794, -2.3442, -2.6116]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpacked.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 6])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpacked.sum(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpacked_out_len.unsqueeze(-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1156, -0.0062,  0.0428,  0.0134,  0.1027,  0.0257],\n",
       "        [-0.6004, -0.5175, -0.4551, -0.4743, -0.3676, -0.4640],\n",
       "        [-0.6893, -0.6172, -0.5902, -0.5162, -0.5894, -0.5147],\n",
       "        [-0.6550, -0.6194, -0.5165, -0.5478, -0.4746, -0.5720],\n",
       "        [-0.6737, -0.6314, -0.5807, -0.5643, -0.5290, -0.5493],\n",
       "        [-0.7875, -0.6421, -0.5386, -0.5759, -0.4688, -0.5223],\n",
       "        [-0.9844, -0.8026, -0.6733, -0.7199, -0.5861, -0.6529]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpacked.sum(1) / unpacked_out_len.unsqueeze(-1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence,pack_padded_sequence,pack_sequence,pad_packed_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data pre-sort: [tensor([7, 8]), tensor([9])]\n",
      "data post-sort: [tensor([7, 8]), tensor([9])]\n",
      "seq_len: [2, 1]\n",
      "data pad_sequence: tensor([[7, 8],\n",
      "        [9, 0]])\n",
      "data pack_padded_sequence: PackedSequence(data=tensor([7, 9, 8]), batch_sizes=tensor([2, 1]), sorted_indices=None, unsorted_indices=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([7, 9, 8]), batch_sizes=tensor([2, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "#def collate_fn(data):\n",
    "#    data.sort(key=lambda x: len(x), reverse=True)\n",
    "#    data = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "#    return data\n",
    "\n",
    "def collate_fn(data):\n",
    "    print(f'data pre-sort: {data}')\n",
    "    data.sort(key=lambda x: len(x), reverse=True)\n",
    "    print(f'data post-sort: {data}')\n",
    "    seq_len = [s.size(0) for s in data] # Get the true length of the data\n",
    "    print(f'seq_len: {seq_len}')\n",
    "    data = pad_sequence(data, batch_first=True)    \n",
    "    print(f'data pad_sequence: {data}')\n",
    "    data = pack_padded_sequence(data, seq_len, batch_first=True)\n",
    "    print(f'data pack_padded_sequence: {data}')\n",
    "    return data\n",
    "\n",
    "\n",
    "a = torch.tensor([1,2,3,4])\n",
    "b = torch.tensor([5,6,7])\n",
    "c = torch.tensor([7,8])\n",
    "d = torch.tensor([9])\n",
    "train_x = [a, b, c, d]\n",
    "\n",
    "data = MyData(train_x)\n",
    "data_loader = DataLoader(data, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "# Use the default collate_fn will report an error\n",
    "#data_loader = DataLoader(data, batch_size=2, shuffle=True) \n",
    "batch_x = iter(data_loader).next()\n",
    "batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([7, 9, 8]), batch_sizes=tensor([2, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.rand(1, 2, 4).float()\n",
    "c0 = torch.rand(1, 2, 4).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7611, 0.8469, 0.3852, 0.3226],\n",
       "         [0.4280, 0.8702, 0.8371, 0.9155]]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(1, 4, batch_first=True)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.RNN(input_size=1, hidden_size=4, num_layers=1, batch_first=True) \n",
    "#h0 = Variable(torch.randn(n_layers, batch_size, hidden_size))\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(4, 4, batch_first=True)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.RNN(input_size=4, hidden_size=4, num_layers=1, batch_first=True) \n",
    "#h0 = Variable(torch.randn(n_layers, batch_size, hidden_size))\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([7, 9, 8]), batch_sizes=tensor([2, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7611, 0.8469, 0.3852, 0.3226],\n",
       "         [0.4280, 0.8702, 0.8371, 0.9155]]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input must have 2 dimensions, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16936\\1694714718.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\SSL-project\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\SSL-project\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mhx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[0m_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_rnn_impls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\SSL-project\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\SSL-project\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mexpected_input_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mexpected_input_dim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m             raise RuntimeError(\n\u001b[0m\u001b[0;32m    202\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[0;32m    203\u001b[0m                     expected_input_dim, input.dim()))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input must have 2 dimensions, got 1"
     ]
    }
   ],
   "source": [
    "#forward \n",
    "out, h1 = rnn(batch_x, h0)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: tensor([[1., 2., 3., 4.],\n",
      "        [5., 6., 7., 0.]])\n",
      "data unsqueeze: tensor([[[1.],\n",
      "         [2.],\n",
      "         [3.],\n",
      "         [4.]],\n",
      "\n",
      "        [[5.],\n",
      "         [6.],\n",
      "         [7.],\n",
      "         [0.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class MyData(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(data):\n",
    "    data.sort(key=lambda x: len(x), reverse=True)\n",
    "    seq_len = [s.size(0) for s in data]\n",
    "    data = torch.nn.utils.rnn.pad_sequence(data, batch_first=True).float()    \n",
    "    print(f\"data: {data}\")\n",
    "    data = data.unsqueeze(-1)\n",
    "    print(f\"data unsqueeze: {data}\")\n",
    "    data = torch.nn.utils.rnn.pack_padded_sequence(data, seq_len, batch_first=True)\n",
    "    return data\n",
    "\n",
    "a = torch.tensor([1,2,3,4])\n",
    "b = torch.tensor([5,6,7])\n",
    "c = torch.tensor([7,8])\n",
    "d = torch.tensor([9])\n",
    "train_x = [a, b, c, d]\n",
    "\n",
    "data = MyData(train_x)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "batch_x = iter(data_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[1.],\n",
       "        [5.],\n",
       "        [2.],\n",
       "        [6.],\n",
       "        [3.],\n",
       "        [7.],\n",
       "        [4.]]), batch_sizes=tensor([2, 2, 2, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h0: torch.Size([1, 2, 4])\n",
      "c0: torch.Size([1, 2, 4])\n",
      "batch_x: torch.Size([7, 1])\n"
     ]
    }
   ],
   "source": [
    "rnn = torch.nn.LSTM(1, 4, 1, batch_first=True)\n",
    "h0 = torch.rand(1, 2, 4).float()\n",
    "c0 = torch.rand(1, 2, 4).float()\n",
    "\n",
    "print(f\"h0: {h0.size()}\")\n",
    "print(f\"c0: {c0.size()}\")\n",
    "print(f\"batch_x: {batch_x.data.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, (h1, c1) = rnn(batch_x, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyData(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "#def collate_fn(data):\n",
    "#    data.sort(key=lambda x: len(x), reverse=True)\n",
    "#    seq_len = [s.size(0) for s in data]\n",
    "#    #print(f\"seq_len: {seq_len}\")\n",
    "#    data = torch.nn.utils.rnn.pad_sequence(data, batch_first=True).float()    \n",
    "#    #print(f\"data.size(): {data.size(1)}\")\n",
    "#    #print(f\"data: {data}\")\n",
    "#    data = data.unsqueeze(-1)\n",
    "#    #print(f\"data unsqueeze: {data}\")\n",
    "#    data = torch.nn.utils.rnn.pack_padded_sequence(data, seq_len, batch_first=True)\n",
    "#    return data\n",
    "\n",
    "def collate_fn(data):\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    seq_len = [s[0].size(0) for s in data]\n",
    "\n",
    "    data_list, label_list = [], []\n",
    "    \n",
    "    for (_data, _label) in data:\n",
    "        data_list.append(_data)\n",
    "        label_list.append(_label)\n",
    "\n",
    "    data_list = torch.nn.utils.rnn.pad_sequence(data_list, batch_first=True, padding_value=-1).float() \n",
    "    #data_list = data_list.unsqueeze(-1)\n",
    "    data_list = torch.nn.utils.rnn.pack_padded_sequence(data_list, seq_len, batch_first=True)\n",
    "\n",
    "    return data_list, label_list\n",
    "\n",
    "\n",
    "a = torch.tensor([[1,2,3], [2,4,3], [3,8,7], [4,6,5]])\n",
    "b = torch.tensor([[1,5,7], [2,6,2], [3,9,10]])\n",
    "c = torch.tensor([[1,64,33], [2,6,14]])\n",
    "d = torch.tensor([[1,20,4]])\n",
    "train_x = [a, b, c, d]\n",
    "train_y = torch.tensor([[0, 1], [1, 0], [1, 0], [0, 1]])\n",
    "\n",
    "\n",
    "data = MyData(train_x, train_y)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "batch_x = iter(data_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PackedSequence(data=tensor([[ 1.,  2.,  3.],\n",
       "         [ 1.,  5.,  7.],\n",
       "         [ 2.,  4.,  3.],\n",
       "         [ 2.,  6.,  2.],\n",
       "         [ 3.,  8.,  7.],\n",
       "         [ 3.,  9., 10.],\n",
       "         [ 4.,  6.,  5.]]), batch_sizes=tensor([2, 2, 2, 1]), sorted_indices=None, unsorted_indices=None),\n",
       " [tensor([0, 1]), tensor([1, 0])])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h0: torch.Size([1, 2, 4])\n",
      "c0: torch.Size([1, 2, 4])\n",
      "batch_x: torch.Size([7, 3])\n"
     ]
    }
   ],
   "source": [
    "rnn = torch.nn.LSTM(3, 4, 1, batch_first=True)\n",
    "h0 = torch.rand(1, 2, 4).float()\n",
    "c0 = torch.rand(1, 2, 4).float()\n",
    "\n",
    "print(f\"h0: {h0.size()}\")\n",
    "print(f\"c0: {c0.size()}\")\n",
    "print(f\"batch_x: {batch_x[0].data.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, (h1, c1) = rnn(batch_x[0], (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 1.9814e-01,  1.7159e-01, -2.6053e-04,  1.8220e-01],\n",
       "        [ 3.6024e-03,  1.6864e-01,  2.1291e-05,  5.1185e-02],\n",
       "        [ 1.0809e-01,  1.6072e-01, -7.2246e-04,  2.1089e-01],\n",
       "        [ 3.7991e-02,  1.0185e-01, -3.0692e-04,  3.1341e-01],\n",
       "        [ 2.8130e-02,  1.5002e-01, -1.2220e-06,  9.3069e-02],\n",
       "        [ 2.6932e-03,  1.4218e-01, -5.3294e-08,  2.9613e-02],\n",
       "        [ 2.1844e-02,  1.3494e-01, -1.5735e-05,  1.8890e-01]],\n",
       "       grad_fn=<CatBackward0>), batch_sizes=tensor([2, 2, 2, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.5786,  7.4177],\n",
       "        [ 2.0741, -2.0567],\n",
       "        [ 2.1463, -1.9977],\n",
       "        [ 1.7627, -1.3743],\n",
       "        [-5.1376,  5.0417],\n",
       "        [ 1.1576, -1.2956],\n",
       "        [-5.2747,  5.0188],\n",
       "        [-5.0353,  4.7304],\n",
       "        [ 2.7630, -2.7475],\n",
       "        [-3.8810,  3.9230]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "prueba_1 = torch.tensor([[-7.5786,  7.4177],\n",
    "        [ 2.0741, -2.0567],\n",
    "        [ 2.1463, -1.9977],\n",
    "        [ 1.7627, -1.3743],\n",
    "        [-5.1376,  5.0417],\n",
    "        [ 1.1576, -1.2956],\n",
    "        [-5.2747,  5.0188],\n",
    "        [-5.0353,  4.7304],\n",
    "        [ 2.7630, -2.7475],\n",
    "        [-3.8810,  3.9230]])\n",
    "\n",
    "prueba_2 = torch.tensor([[32,  43],\n",
    "        [ 14, 54],\n",
    "        [ 64, 76],\n",
    "        [ 76, 87],\n",
    "        [-5.1376,  5.0417],\n",
    "        [ 1.1576, -1.2956],\n",
    "        [-5.2747,  5.0188],\n",
    "        [-5.0353,  4.7304],\n",
    "        [ 2.7630, -2.7475],\n",
    "        [-3.8810,  3.9230]])\n",
    "\n",
    "prueba_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.5786,  7.4177],\n",
       "        [ 2.0741, -2.0567],\n",
       "        [ 2.1463, -1.9977],\n",
       "        [ 1.7627, -1.3743],\n",
       "        [-5.1376,  5.0417],\n",
       "        [ 1.1576, -1.2956],\n",
       "        [-5.2747,  5.0188],\n",
       "        [-5.0353,  4.7304],\n",
       "        [ 2.7630, -2.7475],\n",
       "        [-3.8810,  3.9230],\n",
       "        [32.0000, 43.0000],\n",
       "        [14.0000, 54.0000],\n",
       "        [64.0000, 76.0000],\n",
       "        [76.0000, 87.0000],\n",
       "        [-5.1376,  5.0417],\n",
       "        [ 1.1576, -1.2956],\n",
       "        [-5.2747,  5.0188],\n",
       "        [-5.0353,  4.7304],\n",
       "        [ 2.7630, -2.7475],\n",
       "        [-3.8810,  3.9230]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(([prueba_1, prueba_2]),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0703616130270207e-07"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "math.exp(-7.5786) / (math.exp(-7.5786) + math.exp(7.4177))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(torch.softmax(torch.cat(([prueba_1, prueba_2]),dim=0), dim=0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2348e-05, 7.7962e-01],\n",
       "        [1.9218e-01, 5.9869e-05],\n",
       "        [2.0656e-01, 6.3508e-05],\n",
       "        [1.4075e-01, 1.1846e-04],\n",
       "        [1.4181e-04, 7.2444e-02],\n",
       "        [7.6854e-02, 1.2816e-04],\n",
       "        [1.2364e-04, 7.0803e-02],\n",
       "        [1.5708e-04, 5.3064e-02],\n",
       "        [3.8272e-01, 3.0005e-05],\n",
       "        [4.9823e-04, 2.3668e-02]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(prueba, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.softmax(prueba, dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0704e-07, 1.0000e+00],\n",
       "        [9.8418e-01, 1.5816e-02],\n",
       "        [9.8439e-01, 1.5612e-02],\n",
       "        [9.5839e-01, 4.1607e-02],\n",
       "        [3.7946e-05, 9.9996e-01],\n",
       "        [9.2080e-01, 7.9205e-02],\n",
       "        [3.3851e-05, 9.9997e-01],\n",
       "        [5.7383e-05, 9.9994e-01],\n",
       "        [9.9597e-01, 4.0278e-03],\n",
       "        [4.0793e-04, 9.9959e-01]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(prueba, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[ 2,  6,  8, 35,  6,  8, 98, 56],\n",
       "          [ 2,  6,  8, 35,  6,  8, 98, 56],\n",
       "          [ 2,  6,  8, 35,  6,  8, 98, 56]]),\n",
       "  tensor([0., 1.])),\n",
       " (tensor([[ 9, 64,  8, 35,  6,  8, 98, 56],\n",
       "          [ 7,  5,  8, 35,  6,  8, 98, 56],\n",
       "          [ 5,  6,  8, 35,  6,  8, 98, 56],\n",
       "          [25,  6,  8,  4,  6,  8, 98, 56],\n",
       "          [27,  6,  8, 35,  6,  8, 98, 56],\n",
       "          [28,  6,  8, 35,  6,  8, 98, 56],\n",
       "          [29,  6,  8, 35,  6,  8, 98, 56]]),\n",
       "  tensor([1., 0.])),\n",
       " (tensor([[ 0,  6,  8, 35,  6,  8, 98, 56],\n",
       "          [ 1, 66,  8, 35,  6,  8, 98, 56],\n",
       "          [ 6, 36,  8, 35,  6,  8, 98, 56],\n",
       "          [ 8, 45,  8, 35,  6,  8, 98, 56]]),\n",
       "  tensor([0., 1.]))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pre = [(torch.tensor([[2,6,8,35,6,8,98,56],\n",
    "                           [2,6,8,35,6,8,98,56],\n",
    "                           [2,6,8,35,6,8,98,56]]), torch.tensor([0., 1.])),\n",
    "            (torch.tensor([[9,64,8,35,6,8,98,56],\n",
    "                           [7,5,8,35,6,8,98,56],\n",
    "                           [5,6,8,35,6,8,98,56],\n",
    "                           [25,6,8,4,6,8,98,56],\n",
    "                           [27,6,8,35,6,8,98,56],\n",
    "                           [28,6,8,35,6,8,98,56],\n",
    "                           [29,6,8,35,6,8,98,56]]), torch.tensor([1.,0.])),\n",
    "            (torch.tensor([[0,6,8,35,6,8,98,56],\n",
    "                           [1,66,8,35,6,8,98,56],\n",
    "                           [6,36,8,35,6,8,98,56],\n",
    "                           [8,45,8,35,6,8,98,56]]), torch.tensor([0.,1.]))\n",
    "            ]\n",
    "data_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 7., 4.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_lenght = torch.zeros(len(data_pre))\n",
    "for i in range(len(data_pre)):\n",
    "    list_lenght[i] = len(data_pre[i][0])\n",
    "\n",
    "list_lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_sorted = list_lenght.argsort(descending=True)\n",
    "idx_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7., 4., 3.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = list_lenght[idx_sorted]\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[ 9, 64,  8, 35,  6,  8, 98, 56],\n",
       "          [ 7,  5,  8, 35,  6,  8, 98, 56],\n",
       "          [ 5,  6,  8, 35,  6,  8, 98, 56],\n",
       "          [25,  6,  8,  4,  6,  8, 98, 56],\n",
       "          [27,  6,  8, 35,  6,  8, 98, 56],\n",
       "          [28,  6,  8, 35,  6,  8, 98, 56],\n",
       "          [29,  6,  8, 35,  6,  8, 98, 56]]),\n",
       "  tensor([1., 0.])),\n",
       " (tensor([[ 0,  6,  8, 35,  6,  8, 98, 56],\n",
       "          [ 1, 66,  8, 35,  6,  8, 98, 56],\n",
       "          [ 6, 36,  8, 35,  6,  8, 98, 56],\n",
       "          [ 8, 45,  8, 35,  6,  8, 98, 56]]),\n",
       "  tensor([0., 1.])),\n",
       " (tensor([[ 2,  6,  8, 35,  6,  8, 98, 56],\n",
       "          [ 2,  6,  8, 35,  6,  8, 98, 56],\n",
       "          [ 2,  6,  8, 35,  6,  8, 98, 56]]),\n",
       "  tensor([0., 1.]))]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sorted = []\n",
    "\n",
    "for idx in idx_sorted:\n",
    "    dataset_sorted.append(data_pre[idx])\n",
    "\n",
    "dataset_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.4802,  7.7346],\n",
       "        [ 1.7185, -1.9739],\n",
       "        [ 0.7562, -0.8994]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.tensor([[-7.4802,  7.7346],\n",
    "        [ 1.7185, -1.9739],\n",
    "        [ 0.7562, -0.8994]])\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 0])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_torch = torch.tensor([1, 2, 0])\n",
    "idx_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7185, -1.9739],\n",
       "        [ 0.7562, -0.8994],\n",
       "        [-7.4802,  7.7346]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = scores[idx_sorted]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(size=scores.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.4802,  7.7346],\n",
       "        [ 1.7185, -1.9739],\n",
       "        [ 0.7562, -0.8994]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_original = torch.zeros(size=scores.size())\n",
    "for i in range(len(idx_sorted)):\n",
    "    scores_original[idx_sorted[i]] = scores[i]\n",
    "\n",
    "scores_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = torch.tensor([[0., 1.],\n",
    "        [1., 0.],\n",
    "        [0., 1.]]).to('cuda')\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[idx_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7185, -1.9739],\n",
       "        [ 0.7562, -0.8994],\n",
       "        [-7.4802,  7.7346]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(scores, 0, idx_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "At least one stride in the given numpy array is negative, and tensors with negative strides are not currently supported. (You can probably work around this by making a copy of your array  with array.copy().) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7504/2175209744.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_sorted\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: At least one stride in the given numpy array is negative, and tensors with negative strides are not currently supported. (You can probably work around this by making a copy of your array  with array.copy().) "
     ]
    }
   ],
   "source": [
    "scores[idx_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "At least one stride in the given numpy array is negative, and tensors with negative strides are not currently supported. (You can probably work around this by making a copy of your array  with array.copy().) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24224/2175209744.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_sorted\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: At least one stride in the given numpy array is negative, and tensors with negative strides are not currently supported. (You can probably work around this by making a copy of your array  with array.copy().) "
     ]
    }
   ],
   "source": [
    "scores[idx_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24224/1690510610.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_pre\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_sorted\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "data_pre[idx_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(data_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24224/219990590.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_pre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata_pre\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "sorted(range(len(data_pre)), key=lambda k: data_pre[0][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "labels = torch.tensor([[0., 1.],\n",
    "        [1., 0.],\n",
    "        [0., 1.]])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_pre[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\damor\\AppData\\Local\\Temp/ipykernel_24224/3768877712.py:3: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  data_pre_np = np.array(data_pre, dtype=object)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_pre_np = np.array(data_pre, dtype=object) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argsort() received an invalid combination of arguments - got (key=function, dim=int, ), but expected one of:\n * (Tensor input, int dim, bool descending)\n * (Tensor input, name dim, bool descending)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24224/1304066232.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mdata_pre\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_pre\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: argsort() received an invalid combination of arguments - got (key=function, dim=int, ), but expected one of:\n * (Tensor input, int dim, bool descending)\n * (Tensor input, name dim, bool descending)\n"
     ]
    }
   ],
   "source": [
    "torch.argsort(key=lambda data_pre: len(data_pre[0]), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24224/506285275.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mdata_pre\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_pre\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'function' object is not iterable"
     ]
    }
   ],
   "source": [
    "lambda data_pre: len(data_pre[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key=lambda x: len(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'key' is an invalid keyword argument for argsort()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24224/1496240825.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_pre_np\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'key' is an invalid keyword argument for argsort()"
     ]
    }
   ],
   "source": [
    "data_pre_np.argsort(key=lambda x: len(x[0]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[ 9, 64,  8, 35,  6,  8, 98, 56],\n",
       "          [ 7,  5,  8, 35,  6,  8, 98, 56],\n",
       "          [ 5,  6,  8, 35,  6,  8, 98, 56],\n",
       "          [25,  6,  8,  4,  6,  8, 98, 56],\n",
       "          [27,  6,  8, 35,  6,  8, 98, 56],\n",
       "          [28,  6,  8, 35,  6,  8, 98, 56],\n",
       "          [29,  6,  8, 35,  6,  8, 98, 56]]),\n",
       "  tensor([1., 0.])),\n",
       " (tensor([[ 0,  6,  8, 35,  6,  8, 98, 56],\n",
       "          [ 1, 66,  8, 35,  6,  8, 98, 56],\n",
       "          [ 6, 36,  8, 35,  6,  8, 98, 56],\n",
       "          [ 8, 45,  8, 35,  6,  8, 98, 56]]),\n",
       "  tensor([0., 1.])),\n",
       " (tensor([[ 2,  6,  8, 35,  6,  8, 98, 56],\n",
       "          [ 2,  6,  8, 35,  6,  8, 98, 56],\n",
       "          [ 2,  6,  8, 35,  6,  8, 98, 56]]),\n",
       "  tensor([0., 1.]))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pre_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd4314cc7912fa61485a683f8eef9d94e0afc67b161613bff33a51f804ee9a51"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('SSL-project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
